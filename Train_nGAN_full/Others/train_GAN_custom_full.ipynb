{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ipynb script is using the custom GAN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth enabled for GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "memory_limit = 3072\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)]\n",
    "        )\n",
    "        print(f\"Memory limit set to {memory_limit} MB for GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Load and preprocess data\n",
    "dataset = np.load(\"data_train_766.npy\")\n",
    "x_mean, x_std = np.mean(dataset[:,:,0]), np.std(dataset[:,:,0])\n",
    "y_mean, y_std = np.mean(dataset[:,:,1]), np.std(dataset[:,:,1])\n",
    "\n",
    "# Normalize data\n",
    "dataset_normalized = np.zeros_like(dataset)\n",
    "dataset_normalized[:,:,0] = (dataset[:,:,0] - x_mean) / x_std\n",
    "dataset_normalized[:,:,1] = (dataset[:,:,1] - y_mean) / y_std\n",
    "\n",
    "# Model parameters\n",
    "latent_dim = 100\n",
    "seq_len = 91\n",
    "output_dim = 2\n",
    "\n",
    "generator = keras.Sequential([\n",
    "    keras.layers.Input(shape=(latent_dim,)),\n",
    "    keras.layers.Dense(seq_len * 8, activation='relu'),\n",
    "    keras.layers.Reshape((seq_len, 8)),\n",
    "    keras.layers.LSTM(16, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(output_dim))\n",
    "])\n",
    "\n",
    "\n",
    "discriminator = keras.Sequential([\n",
    "    keras.layers.Input(shape=(seq_len, output_dim)),\n",
    "    keras.layers.LSTM(16),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_images = self.generator(noise, training=True)\n",
    "\n",
    "            real_output = self.discriminator(real_images, training=True)\n",
    "            fake_output = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            gen_loss = self.loss_fn(tf.ones_like(fake_output), fake_output)\n",
    "            disc_loss = self.loss_fn(tf.ones_like(real_output), real_output) + \\\n",
    "                        self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        return {\"d_loss\": disc_loss, \"g_loss\": gen_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0002),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0002),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "\n",
    "gan.fit(dataset_normalized, epochs=1000, batch_size=64)\n",
    "\n",
    "noise = tf.random.normal(shape=(200, latent_dim))\n",
    "generated_samples = gan.generator(noise)\n",
    "\n",
    "generated_samples_denorm = np.zeros_like(generated_samples)\n",
    "generated_samples_denorm[:,:,0] = generated_samples[:,:,0] * x_std + x_mean\n",
    "generated_samples_denorm[:,:,1] = generated_samples[:,:,1] * y_std + y_mean\n",
    "\n",
    "np.save('generated_sample_denorm.npy', generated_samples_denorm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsgm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
