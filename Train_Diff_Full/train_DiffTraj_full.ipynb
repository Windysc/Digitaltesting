{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_228854/379316496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_WD\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = np.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
    "    return emb\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.fc(x)  # shape: (batch_size, num_attributes, 1)\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        return weights\n",
    "\n",
    "\n",
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=256):\n",
    "        super(WideAndDeep, self).__init__()\n",
    "\n",
    "        self.wide_fc = nn.Linear(5, embedding_dim)\n",
    "\n",
    "        self.depature_embedding = nn.Embedding(288, hidden_dim)\n",
    "        self.sid_embedding = nn.Embedding(257, hidden_dim)\n",
    "        self.eid_embedding = nn.Embedding(257, hidden_dim)\n",
    "        self.deep_fc1 = nn.Linear(hidden_dim*3, embedding_dim)\n",
    "        self.deep_fc2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, attr):\n",
    "        continuous_attrs = attr[:, 1:6]\n",
    "\n",
    "        depature, sid, eid = attr[:, 0].long(\n",
    "        ), attr[:, 6].long(), attr[:, 7].long()\n",
    "\n",
    "        wide_out = self.wide_fc(continuous_attrs)\n",
    "\n",
    "        depature_embed = self.depature_embedding(depature)\n",
    "        sid_embed = self.sid_embedding(sid)\n",
    "        eid_embed = self.eid_embedding(eid)\n",
    "        categorical_embed = torch.cat(\n",
    "            (depature_embed, sid_embed, eid_embed), dim=1)\n",
    "        deep_out = F.relu(self.deep_fc1(categorical_embed))\n",
    "        deep_out = self.deep_fc2(deep_out)\n",
    "        combined_embed = wide_out + deep_out\n",
    "\n",
    "        return combined_embed\n",
    "\n",
    "\n",
    "def nonlinearity(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return torch.nn.GroupNorm(num_groups=32,\n",
    "                              num_channels=in_channels,\n",
    "                              eps=1e-6,\n",
    "                              affine=True)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            self.conv = torch.nn.Conv1d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.interpolate(x,\n",
    "                                            scale_factor=2.0,\n",
    "                                            mode=\"nearest\")\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv=True):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            self.conv = torch.nn.Conv1d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=2,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.with_conv:\n",
    "            pad = (1, 1)\n",
    "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "            x = self.conv(x)\n",
    "        else:\n",
    "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels=None,\n",
    "                 conv_shortcut=False,\n",
    "                 dropout=0.1,\n",
    "                 temb_channels=512):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
    "        self.norm2 = Normalize(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = torch.nn.Conv1d(out_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                self.conv_shortcut = torch.nn.Conv1d(in_channels,\n",
    "                                                     out_channels,\n",
    "                                                     kernel_size=3,\n",
    "                                                     stride=1,\n",
    "                                                     padding=1)\n",
    "            else:\n",
    "                self.nin_shortcut = torch.nn.Conv1d(in_channels,\n",
    "                                                    out_channels,\n",
    "                                                    kernel_size=1,\n",
    "                                                    stride=1,\n",
    "                                                    padding=0)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "        h = h + self.temb_proj(nonlinearity(temb))[:, :, None]\n",
    "        h = self.norm2(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                x = self.conv_shortcut(x)\n",
    "            else:\n",
    "                x = self.nin_shortcut(x)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv1d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv1d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv1d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv1d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "        b, c, w = q.shape\n",
    "        q = q.permute(0, 2, 1) \n",
    "        w_ = torch.bmm(q, k)  \n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "        w_ = w_.permute(0, 2, 1)  \n",
    "        h_ = torch.bmm(v, w_)\n",
    "        h_ = h_.reshape(b, c, w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x + h_\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        ch, out_ch, ch_mult = config.model.ch, config.model.out_ch, tuple(\n",
    "            config.model.ch_mult)\n",
    "        num_res_blocks = config.model.num_res_blocks\n",
    "        attn_resolutions = config.model.attn_resolutions\n",
    "        dropout = config.model.dropout\n",
    "        in_channels = config.model.in_channels\n",
    "        resolution = config.data.traj_length\n",
    "        resamp_with_conv = config.model.resamp_with_conv\n",
    "        num_timesteps = config.diffusion.num_diffusion_timesteps\n",
    "\n",
    "        if config.model.type == 'bayesian':\n",
    "            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n",
    "\n",
    "        self.ch = ch\n",
    "        self.temb_ch = self.ch * 4\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.temb = nn.Module()\n",
    "        self.temb.dense = nn.ModuleList([\n",
    "            torch.nn.Linear(self.ch, self.temb_ch),\n",
    "            torch.nn.Linear(self.temb_ch, self.temb_ch),\n",
    "        ])\n",
    "\n",
    "        self.conv_in = torch.nn.Conv1d(in_channels,\n",
    "                                       self.ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1, ) + ch_mult\n",
    "        self.down = nn.ModuleList()\n",
    "        block_in = None\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch * in_ch_mult[i_level]\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(\n",
    "                    ResnetBlock(in_channels=block_in,\n",
    "                                out_channels=block_out,\n",
    "                                temb_channels=self.temb_ch,\n",
    "                                dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(AttnBlock(block_in))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "            self.down.append(down)\n",
    "\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = AttnBlock(block_in)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            skip_in = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                if i_block == self.num_res_blocks:\n",
    "                    skip_in = ch * in_ch_mult[i_level]\n",
    "                block.append(\n",
    "                    ResnetBlock(in_channels=block_in + skip_in,\n",
    "                                out_channels=block_out,\n",
    "                                temb_channels=self.temb_ch,\n",
    "                                dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(AttnBlock(block_in))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "            self.up.insert(0, up) \n",
    "\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv1d(block_in,\n",
    "                                        out_ch,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x, t, extra_embed=None):\n",
    "        assert x.shape[2] == self.resolution\n",
    "\n",
    "        temb = get_timestep_embedding(t, self.ch)\n",
    "        temb = self.temb.dense[0](temb)\n",
    "        temb = nonlinearity(temb)\n",
    "        temb = self.temb.dense[1](temb)\n",
    "        if extra_embed is not None:\n",
    "            temb = temb + extra_embed\n",
    "\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        h = hs[-1]  # [10, 256, 4, 4]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                ht = hs.pop()\n",
    "                if ht.size(-1) != h.size(-1):\n",
    "                    h = torch.nn.functional.pad(h,\n",
    "                                                (0, ht.size(-1) - h.size(-1)))\n",
    "                h = self.up[i_level].block[i_block](torch.cat([h, ht], dim=1),\n",
    "                                                    temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Guide_UNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Guide_UNet, self).__init__()\n",
    "        self.config = config\n",
    "        self.ch = config.model.ch * 4\n",
    "        self.attr_dim = config.model.attr_dim\n",
    "        self.guidance_scale = config.model.guidance_scale\n",
    "        self.unet = Model(config)\n",
    "        self.guide_emb = WideAndDeep(self.ch)\n",
    "        self.place_emb = WideAndDeep(self.ch)\n",
    "\n",
    "    def forward(self, x, t, attr):\n",
    "        guide_emb = self.guide_emb(attr)\n",
    "        place_vector = torch.zeros(attr.shape, device=attr.device)\n",
    "        place_emb = self.place_emb(place_vector)\n",
    "        cond_noise = self.unet(x, t, guide_emb)\n",
    "        uncond_noise = self.unet(x, t, place_emb)\n",
    "        pred_noise = cond_noise + self.guidance_scale * (cond_noise -\n",
    "                                                         uncond_noise)\n",
    "        return pred_noise\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from utils.config_WD import args\n",
    "\n",
    "    temp = {}\n",
    "    for k, v in args.items():\n",
    "        temp[k] = SimpleNamespace(**v)\n",
    "\n",
    "    config = SimpleNamespace(**temp)\n",
    "    t = torch.randn(10)\n",
    "    depature = torch.zeros(10)\n",
    "    avg_dis = torch.zeros(10)\n",
    "    avg_speed = torch.zeros(10)\n",
    "    total_dis = torch.zeros(10)\n",
    "    total_time = torch.zeros(10)\n",
    "    total_len = torch.zeros(10)\n",
    "    sid = torch.zeros(10)\n",
    "    eid = torch.zeros(10)\n",
    "    attr = torch.stack(\n",
    "        [depature, total_dis, total_time, total_len, avg_dis, avg_speed, sid, eid], dim=1)\n",
    "    unet = Guide_UNet(config)\n",
    "    x = torch.randn(10, 2, 200)\n",
    "    total_params = sum(p.numel() for p in unet.parameters())\n",
    "    print(f'{total_params:,} total parameters.')\n",
    "    out = unet(x, t, attr)\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junze/anaconda3/envs/dt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Traj_Unet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_228854/322996371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleNamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mTraj_Unet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGuide_UNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_WD\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Traj_Unet'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from Traj_Unet import Guide_UNet\n",
    "from utils.config_WD import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(**{k: SimpleNamespace(**v) for k, v in args.items()})\n",
    "\n",
    "def prepare_data(data):\n",
    "    data = np.transpose(data, (0, 2, 1))\n",
    "    \n",
    "    attr = np.zeros((data.shape[0], 8))\n",
    "    \n",
    "    return torch.FloatTensor(data), torch.FloatTensor(attr)\n",
    "\n",
    "data = np.random.randn(766, 91, 2)  \n",
    "trajectories, attributes = prepare_data(data)\n",
    "\n",
    "dataset = TensorDataset(trajectories, attributes)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "model = Guide_UNet(config)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_trajectories, batch_attributes in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        t = torch.randint(0, config.diffusion.num_diffusion_timesteps, (batch_trajectories.shape[0],))\n",
    "        \n",
    "        output = model(batch_trajectories, t, batch_attributes)\n",
    "        \n",
    "        loss = criterion(output, batch_trajectories)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "\n",
    "def generate_trajectory(model, attr, num_steps=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(1, 2, 91)\n",
    "        \n",
    "        for t in reversed(range(num_steps)):\n",
    "            t_tensor = torch.full((1,), t, dtype=torch.long)\n",
    "            pred_noise = model(x, t_tensor, attr)\n",
    "            x = x - pred_noise \n",
    "    \n",
    "    return x.squeeze().numpy()\n",
    "\n",
    "new_attr = torch.zeros(1, 8)\n",
    "generated_traj = generate_trajectory(model, new_attr)\n",
    "print(\"Generated trajectory shape:\", generated_traj.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsgm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
